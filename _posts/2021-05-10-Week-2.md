---
layout: post
title: Logistic regression as a neural network
published: true
---
W2 of Neural Networks and Deep Learning by Andrew Ng.

## Binary classification
Consider that we have an input 64p x 64p image and want to output a binary classification label corresponding to that image (e.g. cat or not a cat etc.). The image is represented as an RGB so we will have 3 64 x 64 matrices corresponding to the R G B intensity values. To unroll these feature intensity matrices into a feature vector, we simply define a new vector containing all values in the 3 matrices (so the dimension of the input feature vector, <img src="https://latex.codecogs.com/gif.latex?%5Cinline%20n_x"> will be 3 * 64 * 64 = 12288). The network takes in that input feature vector, `x` and outputs a label, `y`. 

## Notation
A single training example is `(x,y)` where `x` <img src="https://latex.codecogs.com/gif.latex?%5Cin%20%5Cmathbb%7BR%7D%5En_x"> and `y` <img src="https://latex.codecogs.com/gif.latex?%5Cin"> `{0,1}`. 

The training set comprises of `m` training examples: <img src="https://latex.codecogs.com/gif.latex?%28x%5E1%2C%20y%5E1%29%2C%20%28x%5E2%2C%20y%5E2%29%2C...%2C%28x%5Em%2C%20y%5Em%29">. So, <img src="https://latex.codecogs.com/gif.latex?m_%7Btest%7D%2C%20m_%7Btrain%7D"> represent the number of testing and training examples, respectively. <img src = "https://latex.codecogs.com/gif.latex?X%20%3D%20%5Cbegin%7Bbmatrix%7D%20%7C%20%26%7C%20%26%7C%20%5C%5C%20x%5E1%20%26%20...%20%26x%5Em%20%5C%5C%20%7C%20%26%20%7C%20%26%7C%20%5Cend%7Bbmatrix%7D%20">

<img src="https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D%20y%5E1%20%26%20y%5E2%20%26...%20%26y%5Em%20%5Cend%7Bbmatrix%7D">

Summary: `X` is a <img src="https://latex.codecogs.com/gif.latex?%5Cinline%20n_x"> by `m` dimensional matrix. `Y` is a `1` by `m` dimensional matrix.

## Logistic regression
Given `x`, we want <img src ="https://latex.codecogs.com/gif.latex?%5Chat%7By%7D%20%3D%20P%28y%20%3D%201%20%7C%20x%29">

`x` is a <img src = "https://latex.codecogs.com/gif.latex?%5Cinline%20n_x"> dimensional vector.

Parameters of logistic regression would be weight which is also a <img src = "https://latex.codecogs.com/gif.latex?%5Cinline%20n_x"> dimensional vector and bias which is a real number. 

Since the logistic regression output <img src ="https://latex.codecogs.com/gif.latex?%5Chat%7By%7D%20%3D%20P%28y%20%3D%201%20%7C%20x%29"> has to be between (0,1), we can use the sigmoid function (Fig 1) and represent it as follows:

<img src = "https://latex.codecogs.com/gif.latex?%5Csigma%28z%29%20%3D%20%5Cfrac%7B1%7D%7B1&plus;e%5E%7B-z%7D%7D">
<img src = "https://latex.codecogs.com/gif.latex?%5Chat%7By%7D%20%3D%20%5Csigma%20%28w%5ETx%20&plus;%20b%29">

<img src ="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png">

If `z` is large, the sigmoid of `z` would be close to 1 and if `z` is small, the sigmoid of `z` would be close to 0.

Our goal while training a logistic regression model is to choose good hyperparameters, such that <img src ="https://latex.codecogs.com/gif.latex?%5Chat%7By%7D"> becomes a good representation of `x`.

## Logistic regression cost function

Loss function, <img src = "https://latex.codecogs.com/gif.latex?L%28%5Chat%7By%7D%2Cy%29">, to measure how well the algorithm performs. We can use RMSE, MSE, etc but in logistic regression since the optimization problem can become non-convex (multiple local minima), these won't work well. Gradient descent may not find the global min if we use these. 

<img src = "https://latex.codecogs.com/gif.latex?%5Clarge%20L%28%5Chat%7By%7D%2Cy%29%20%3D%20-%28y%20log%28%5Chat%7By%7D%29%20&plus;%20%281-y%29log%281-%5Chat%7By%7D%29%29">

We want the loss function to be as small as possible. 
If `y` is 1: <img src = "https://latex.codecogs.com/gif.latex?L%28%5Chat%7By%7D%2Cy%29%20%3D%20-log%28%5Chat%7By%7D%29"> (loss function is trying to make <img src ="https://latex.codecogs.com/gif.latex?%5Chat%7By%7D"> as large as possible (i.e. close to 1)).
if `y` is 0: <img src ="https://latex.codecogs.com/gif.latex?L%28%5Chat%7By%7D%2Cy%29%20%3D%20-log%281-%5Chat%7By%7D%29"> (loss function is trying to make <img src ="https://latex.codecogs.com/gif.latex?%5Chat%7By%7D"> as small as possible (i.e. close to 0)).

Here, loss function is applied to a single training example, while the cost function average of the loss functions for the entire training set. It is the cost of parameters, `w` and `b`. In the logistic regression model, we will try to find parameters that minimize the cost function, `J(w, b)`, defined as follows:

<img src = "https://latex.codecogs.com/gif.latex?J%28w%2Cb%29%20%3D%20-%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7DL%28%5Chat%7By%7D%5E%7B%28i%29%7D%2C%20y%5E%7B%28i%29%7D%29%29">

and we know from before that <img src = "https://latex.codecogs.com/gif.latex?%5Clarge%20L%28%5Chat%7By%7D%2Cy%29%20%3D%20-%28y%20log%28%5Chat%7By%7D%29%20+%20%281-y%29log%281-%5Chat%7By%7D%29%29">

A logistic regression can be viewed as a small NN.
