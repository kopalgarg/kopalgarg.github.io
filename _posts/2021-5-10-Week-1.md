---
layout: post
title: Deep learning
published: true
---
W1 of Neural Networks and Deep Learning by Andrew Ng.

## What is a Neural Network?
Linear regression is similar to a single neuron Neural Network (NN). A larger NN is formed by stacking several such single neurons together. Example: We're trying to predict the price of a house based on features like zip code (may be indicative of _neighborhood walkability_), size of house & # bedrooms (depending on ones _family size_ they may require a larger house, or more bedrooms), wealth (may be an indirect measure of _school quality_). Based on each of these italisized phrases, we can estimate the price, `Y`. In this example, `X` represents all 4 features: zip code, size of house, # bedrooms and wealth. Given these input features, the job of the NN is to predict the price of the house. Every input layer feature is interconnected with every hidden layer features, and the hidden layer's job is to map from `X` to `Y`. As seen in this example, NN can be useful for supervised learning.

## Supervised Learning with Neural Networks
In supervised learning, we have some input `X` which maps to some input `Y`. Examples:

1) In online advertising, by inputting an advertisement and some information about a user, NN can predict whether the user will click on the ad or not `(0/1)`

2) Tagging an input image as a recognized object `(1,...,1000)`

3) Inputting an audio clip and outputting a text transcript

4) Inputting text in one language and outputting the text translation to another language, etc. 

For the real-estate example and e.g. 1), we may use a standard NN. For e.g. 2), we may have to use a convolutional neural network (CNN). In e.g. 3) the input is in form of an audio which is best represented as a 1D temporal sequence. We may use a recurrent neural network (RNN). Language is also naturally represented as seqeunce data (words and characters are read sequentially), and also require more complex versions of RNNs. For autonomous driving with images and radar information as input, we may require a custom/hybrid NN architecture.  

Summary: Standard NN for standard use cases, CNN for images, RNN + more advanced architectures for sequential data. 

## Supervised learning for structures and unstructured data
In structured data, each feature (like size, # bedrooms, etc.) has a well-defined meaning and can be categorical, or numerical. Images, audio or text, are examples of unstructured data where the input features are pixel values in an image, or individual characters/words in a piece of text, etc. 

## Scale drives DL progress
In the recent years, scale (both in the number of hidden units & layers and training examples `(x,y)`) has been driving DL progress. For high performance, we need to train a large NN and use huge amounts of _labelled_ data, `m`. Its also important to rememebr that large networks take more computational resources and time to train. 

Scale is not limited to data and computation. Recently, an increading numeber of algorithmic innovations are being introduced. They aim at making the NN train much faster. For example, using a Rectified Linear Unit (ReLU) function instead of the sigmoid activation function. Sigmoid function has a gradient close to 0 on either ends, while a ReLU has a gradient 1 for all positive values of input, so the gradient is much less likely to gradually restrain to 0. This makes the gradient descent algorithm work much faster.
